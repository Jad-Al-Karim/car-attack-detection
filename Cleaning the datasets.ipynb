{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1096fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import csv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050bd95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess individual attack datasets\n",
    "df_dos = pd.read_csv('DoS_dataset.csv')\n",
    "print(\"DoS Dataset:\")\n",
    "print(df_dos.head())\n",
    "print(df_dos.info())\n",
    "print(df_dos.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e255d95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Gear dataset\n",
    "df_gear = pd.read_csv('gear_dataset.csv')\n",
    "print(\"Gear Dataset:\")\n",
    "print(df_gear.head())\n",
    "print(df_gear.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12a017a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load RPM dataset\n",
    "df_rpm = pd.read_csv('RPM_dataset.csv')\n",
    "print(\"RPM Dataset:\")\n",
    "print(df_rpm.head())\n",
    "print(df_rpm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7daebed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Fuzzy dataset\n",
    "df_fuzzy = pd.read_csv('Fuzzy_dataset.csv')\n",
    "print(\"Fuzzy Dataset:\")\n",
    "print(df_fuzzy.head())\n",
    "print(df_fuzzy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951be214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process normal_run_data.txt file\n",
    "print(\"Reading normal_run_data.txt...\")\n",
    "df_normal = pd.read_csv('normal_run_data.txt', sep='\\s+', header=None, skiprows=1)\n",
    "\n",
    "# Select the correct columns using the compressed indices\n",
    "# Index 1: Timestamp Value, Index 3: ID Value, Index 5: DLC Value, Index 6-13: Data Bytes\n",
    "df_normal = df_normal.iloc[:, [1, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13]]\n",
    "\n",
    "# Rename to standard 11 columns\n",
    "df_normal.columns = ['Timestamp', 'ID', 'DLC', 'D0', 'D1', 'D2', 'D3', 'D4', 'D5', 'D6', 'D7']\n",
    "\n",
    "# Add the Label column\n",
    "df_normal['Label'] = 'Normal'\n",
    "\n",
    "# Save to a new CSV file\n",
    "output_filename = 'normal_dataset_clean_withR.csv'\n",
    "df_normal.to_csv(output_filename, index=False)\n",
    "print(f\"Success! Saved {len(df_normal)} normal records to {output_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feed8397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify cleaned normal dataset\n",
    "df_normal_check = pd.read_csv(\"normal_dataset_clean_withR.csv\")\n",
    "print(f\"Normal dataset shape: {df_normal_check.shape}\")\n",
    "print(df_normal_check.head())\n",
    "print(df_normal_check.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0c034f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all datasets into master dataset\n",
    "files_to_merge = [\n",
    "    ('normal_dataset_clean_withR.csv', 'Normal'),\n",
    "    ('DoS_dataset.csv', 'DoS'),\n",
    "    ('Fuzzy_dataset.csv', 'Fuzzy'),\n",
    "    ('RPM_dataset.csv', 'RPM'),\n",
    "    ('Gear_dataset.csv', 'Gear')\n",
    "]\n",
    "\n",
    "all_data = []\n",
    "final_columns = ['Timestamp', 'ID', 'DLC', 'D0', 'D1', 'D2', 'D3', 'D4', 'D5', 'D6', 'D7', 'Label']\n",
    "\n",
    "print(\"Starting robust merge...\")\n",
    "\n",
    "for file_name, attack_type in files_to_merge:\n",
    "    print(f\"\\nProcessing {file_name}...\")\n",
    "    \n",
    "    try:\n",
    "        # If it's the CLEAN Normal file - it has headers\n",
    "        if 'normal_dataset' in file_name:\n",
    "            df = pd.read_csv(file_name, on_bad_lines='skip')\n",
    "            \n",
    "            # Ensure it has the right columns\n",
    "            if len(df.columns) == 12:\n",
    "                df.columns = final_columns\n",
    "            \n",
    "        # If it's a RAW Attack CSV - NO headers, assign them manually\n",
    "        else:\n",
    "            df = pd.read_csv(file_name, header=None, on_bad_lines='skip')\n",
    "            \n",
    "            # Temporary names to process the Flag\n",
    "            df.columns = ['Timestamp', 'ID', 'DLC', 'D0', 'D1', 'D2', 'D3', 'D4', 'D5', 'D6', 'D7', 'Flag']\n",
    "            \n",
    "            # Convert Flag ('R'/'T') to Label\n",
    "            df['Label'] = df['Flag'].apply(lambda x: 'Normal' if x == 'R' else attack_type)\n",
    "            \n",
    "            # Drop Flag so we have exactly 12 columns\n",
    "            df.drop(columns=['Flag'], inplace=True)\n",
    "            \n",
    "            # Align column names exactly\n",
    "            df.columns = final_columns\n",
    "\n",
    "        print(f\" -> Successfully loaded {len(df)} rows.\")\n",
    "        all_data.append(df)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"!!! Error loading {file_name}: {e}\")\n",
    "\n",
    "# Combine all loaded frames\n",
    "if all_data:\n",
    "    print(\"\\nConcatenating...\")\n",
    "    final_df = pd.concat(all_data, ignore_index=True)\n",
    "    \n",
    "    # Save the Master Dataset\n",
    "    output_file = 'master_dataset_final.csv'\n",
    "    final_df.to_csv(output_file, index=False)\n",
    "    print(f\"DONE! Saved {len(final_df)} records to {output_file}\")\n",
    "    print(\"\\nClass Distribution:\")\n",
    "    print(final_df['Label'].value_counts())\n",
    "else:\n",
    "    print(\"No data was loaded. Please check your file paths.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3181fe67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify final master dataset\n",
    "df_master = pd.read_csv('master_dataset_final.csv')\n",
    "print(f\"Master dataset shape: {df_master.shape}\")\n",
    "print(df_master.head())\n",
    "print(\"\\nData types:\")\n",
    "print(df_master.dtypes)\n",
    "print(\"\\nNull values:\")\n",
    "print(df_master.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368ea436",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f02c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1. Load the data\n",
    "print(\"Loading master dataset...\")\n",
    "df = pd.read_csv('master_dataset_final.csv', low_memory=False)\n",
    "\n",
    "# 2. Define a safer conversion function\n",
    "def safe_hex_to_int(x):\n",
    "    # If the value is missing/NaN, return 0 (or None if you plan to drop later)\n",
    "    if pd.isnull(x):\n",
    "        return None\n",
    "    \n",
    "    # Convert to string to handle Floats (e.g., 8.0 -> \"8.0\")\n",
    "    s = str(x).strip()\n",
    "    \n",
    "    # Remove the decimal point if pandas added it (e.g., \"8.0\" -> \"8\")\n",
    "    if '.' in s:\n",
    "        s = s.split('.')[0]\n",
    "        \n",
    "    # If the string is empty or 'nan', return None\n",
    "    if s == '' or s.lower() == 'nan':\n",
    "        return None\n",
    "        \n",
    "    try:\n",
    "        # Convert Hex string to Integer (Base 16)\n",
    "        return int(s, 16)\n",
    "    except ValueError:\n",
    "        # If it's pure garbage data, return None so we can drop it later\n",
    "        return None\n",
    "\n",
    "# 3. Apply the fix\n",
    "hex_columns = ['ID', 'D0', 'D1', 'D2', 'D3', 'D4', 'D5', 'D6', 'D7']\n",
    "print(\"Converting Hex to Integers (Robust Mode)...\")\n",
    "\n",
    "for col in hex_columns:\n",
    "    print(f\"Processing {col}...\")\n",
    "    df[col] = df[col].apply(safe_hex_to_int)\n",
    "\n",
    "# 4. Drop Nulls (Cleaning Step)\n",
    "print(\"Dropping rows with corrupted/missing values...\")\n",
    "initial_count = len(df)\n",
    "df.dropna(inplace=True)\n",
    "print(f\"Dropped {initial_count - len(df)} rows.\")\n",
    "\n",
    "# 5. Label Encoding (Turning 'DoS', 'Normal' into numbers)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "df['Label_Encoded'] = encoder.fit_transform(df['Label'])\n",
    "\n",
    "# 6. Save the clean numeric file\n",
    "output_file = 'final_clean_numeric_dataset.csv'\n",
    "df.to_csv(output_file, index=False)\n",
    "print(f\"Success! Cleaned numeric dataset saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee31e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df=pd.read_csv(\"final_clean_numeric_dataset.csv\")\n",
    "print(df['Label'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49dfc3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. Load the clean dataset\n",
    "filename = 'final_clean_numeric_dataset.csv'\n",
    "print(f\"Loading {filename}...\")\n",
    "df = pd.read_csv(filename)\n",
    "\n",
    "# 2. Check the ACTUAL counts after cleaning\n",
    "class_counts = df['Label_Encoded'].value_counts()\n",
    "print(\"\\nActual Class Counts:\")\n",
    "print(class_counts)\n",
    "\n",
    "# 3. Determine the limit automatically\n",
    "# We take the count of the smallest class\n",
    "min_class_size = class_counts.min()\n",
    "print(f\"\\nSmallest class has {min_class_size} samples.\")\n",
    "print(f\"Balancing all classes to {min_class_size}...\")\n",
    "\n",
    "# 4. Balanced Sampling\n",
    "# This uses 'min_class_size' so it will never crash\n",
    "balanced_df = df.groupby('Label_Encoded').apply(lambda x: x.sample(n=min_class_size, random_state=42))\n",
    "\n",
    "# 5. Reset index and Verify\n",
    "balanced_df = balanced_df.reset_index(drop=True)\n",
    "print(\"\\nNew Balanced Distribution:\")\n",
    "print(balanced_df['Label_Encoded'].value_counts())\n",
    "\n",
    "# 6. Save\n",
    "output_file = 'balanced_training_data_final.csv'\n",
    "balanced_df.to_csv(output_file, index=False)\n",
    "print(f\"\\nSuccess! Balanced dataset saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df70dd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1. Load your balanced data\n",
    "print(\"Loading balanced dataset...\")\n",
    "df = pd.read_csv('balanced_training_data_final.csv', low_memory=False)\n",
    "\n",
    "# 2. Define all columns that MUST be numbers\n",
    "# (Everything except the Label)\n",
    "numeric_cols = ['Timestamp', 'ID', 'DLC', 'D0', 'D1', 'D2', 'D3', 'D4', 'D5', 'D6', 'D7']\n",
    "\n",
    "print(\"Scrubbing non-numeric text (like 'DLC:', 'ID:', etc)...\")\n",
    "for col in numeric_cols:\n",
    "    # pd.to_numeric with errors='coerce' turns \"DLC:\" into NaN (Null)\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "# 3. Check how many bad rows we found\n",
    "initial_count = len(df)\n",
    "df.dropna(inplace=True)\n",
    "dropped_count = initial_count - len(df)\n",
    "\n",
    "print(f\"Found and removed {dropped_count} rows containing text errors.\")\n",
    "print(f\"Final clean shape: {df.shape}\")\n",
    "\n",
    "# 4. Save it so you never face this error again\n",
    "df.to_csv('final_train_ready_scrubbed.csv', index=False)\n",
    "print(\"Saved to 'final_train_ready_scrubbed.csv'. Ready for training!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
